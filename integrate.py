## see helpers/helpers.py for warning flag table
import numpy, argparse, pandas, scipy, os
from helpers import helpers

def parse_pair(s):
    return [float(x) for x in s.split(',')]

parser = argparse.ArgumentParser()
parser.add_argument("--input_file", type=str, help="parquet file generated by variable_creation.py")
parser.add_argument("--output_file", type=str, help="name of output file, with path.")
parser.add_argument("--variable", type=str, help="variable to compute")
parser.add_argument('--region', type=parse_pair, required=True,help='An integration region denoted as shallow,deep.')
parser.add_argument("--pressure_buffer", type=float, nargs='?', const=100.0, default=100.0, help="pressure range to keep on either side of the pressure ROI")
parser.add_argument("--pressure_index_buffer", type=int, nargs='?', const=5, default=5, help="minimum number of elements to preserve in the pressure buffer margins")
args = parser.parse_args()

df = pandas.read_parquet(args.input_file, engine='pyarrow')

# create a dense comb for trapezoidal integration
pressure_comb = helpers.integration_comb(args.region)

# temperatures should be in kelvin before integration
variable = args.variable
if variable in ['temperature', 'conservative_temperature', 'potential_temperature']:
    df[variable] = df[variable] + 273.15

# interpolate to comb
df[[variable+'_comb', 'flag']] = df.apply(
    lambda row: pandas.Series(helpers.interpolate_to_levels(row, variable, pressure_comb)),
    axis=1
)

# integrate over region
df[variable+'_integration'] = df.apply(
    lambda row: helpers.integration_region(args.region, pressure_comb, row[variable+'_comb']),
    axis=1
)

# combs can be huge, drop them.
df = df.drop(columns=[variable+'_comb'])

# dump any rows that failed to integrate
rejects = df[df[variable+'_integration'].apply(lambda x: numpy.isnan(x[0]) )].reset_index(drop=True)
rejects = rejects[['float', 'cycle', 'longitude', 'latitude', 'juld', 'flag']]
df = df[~df[variable+'_integration'].apply(lambda x: numpy.isnan(x[0]) )].reset_index(drop=True)

rejects.to_parquet(os.path.join(args.output_file.split('.')[0] + '_rejects.parquet'), engine='pyarrow')
df.to_parquet(args.output_file, engine='pyarrow')
